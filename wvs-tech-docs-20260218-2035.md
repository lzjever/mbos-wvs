# Workspace Versioning Service (WVS) 技术方案文档

## Executive Summary

本文档是 **Workspace Versioning Service（WVS）** 的完整技术方案，提供面向 AI agent 工作目录的 **快照、回滚与 current 切换** 能力。  
MVP 最小栈：**JuiceFS 社区版 + MinIO + PostgreSQL + wvs-api/wvs-worker/executor + vmagent/vmsingle + 控制台 UXUI（CLI/TUI）**；  
GA 扩展栈：**VictoriaMetrics 扩展链路（vmauth/vmalert）+ Grafana + PITR 编排 + 生产化安全与发布体系**。

核心设计：把 JuiceFS 的 `clone`（元数据级克隆）抽象成 "snapshot 集合 + current 指针 + 任务状态机"，通过 **wvs-api（控制面）+ wvs-worker（调度面）+ executor（执行面）** 三层解耦；agent 只需使用挂载后的 POSIX 目录，不直接调用 clone。

## 规范性术语（强制）

本文档使用以下规范性术语：

- `MUST` / `必须`：必须实现，不允许偏离
- `MUST NOT` / `禁止`：绝对禁止
- `SHOULD` / `应`：默认必须执行；仅在有书面豁免时允许偏离
- `MAY` / `可`：可选能力，不影响合规

若同一主题存在冲突，以“更严格约束”为准；若仍冲突，优先级如下：
1. `docs/04-api-and-protocols.md`（外部契约）
2. `docs/05-data-model-ddl.md`（持久化契约）
3. `docs/06-task-engine.md`（状态机与并发语义）
4. 其他章节（实现与运维说明）

## 范围冻结与变更门禁（强制）

以下项在 MVP 阶段冻结，开发团队不得自行扩展：

1. 仅支持单租户逻辑域（`tenant` 仅 GA 引入）
2. 仅支持单 JuiceFS volume
3. 仅支持目录级版本控制（禁止 diff/merge）
4. 仅支持 API/CLI（禁止自建 Web UI 需求变体）
   必须提供控制台 UXUI（CLI/TUI）覆盖主要功能流程
5. 所有涉及 clone 的写操作必须异步任务化（禁止同步执行 clone）

任何新增能力（新 API、新状态、新存储依赖）必须先修改本技术文档并评审通过，再允许编码。

## 分层实施策略（强制）

本文档分为两层执行：

1. `MVP 必选（阻塞项）`：不满足则不得宣称 MVP 完成
2. `GA 扩展（非阻塞项）`：可并行设计，但不得阻塞 MVP 开发与交付

章节分层约定：

- `docs/00~07` 默认按 MVP 执行（除章节内明确标注 GA 的小节）
- `docs/08`（PITR）、`docs/10`（vmalert 规则）、`docs/11`（Grafana 看板）属于 GA 扩展
- `docs/12` 中 K8s 生产清单、生产发布门禁属于 GA 扩展；MVP 仅要求 docker-compose 与基础 CI

## 实施完成定义（DoD，强制）

每个功能项完成必须同时满足：

1. API 合约测试通过（请求/响应/错误码与本文档一致）
2. DDL 与迁移文件一致（无“文档字段存在但库表缺失”）
3. 任务状态机测试覆盖成功、失败、重试、取消、超时路径
4. 审计日志字段完整（`request_id/task_id/wsid/actor/action`）
5. MVP：指标与日志可关联到同一 `request_id`，且 vmagent/vmsingle 可用
6. MVP：必须提供控制台 UXUI 演示全主流程（workspace/snapshot/set_current/drop/task）

## 关键技术事实与约束

以下事实来自官方文档，是本方案的技术基础。**开发团队必须理解这些约束后再开始编码。**

分层说明：
- 与 clone 一致性、任务并发、幂等相关事实属于 MVP 必选约束
- 与 PITR、Object Lock、高级观测链路相关事实属于 GA 约束，不阻塞 MVP

### JuiceFS 侧

| 事实 | 影响 |
|---|---|
| `juicefs clone` 是元数据级操作，不拷贝对象存储数据，通过 CoW 保持源与克隆独立 | clone 速度快，但会增加元数据引擎占用 |
| **clone 对目录不是原子操作**——如果 clone 过程中源目录有写入，目标可能不一致 | **quiesce 不是可选的，set_current/snapshot 前必须暂停 agent 写入** |
| clone 操作通过 **FUSE 挂载路径**执行（如 `/mnt/jfs/src /mnt/jfs/dst`），由 mount 进程完成 | executor 必须拥有活跃的 JuiceFS FUSE 挂载 |
| 同一源同一时刻只能有一个 clone 操作 | worker 层的 workspace 级互斥锁已保证此约束 |
| clone 被中断时，mount 进程会清理已创建的 inode | 不需要额外的 clone 中断清理逻辑 |
| mount 暴露 Prometheus 指标于 `http://localhost:9567/metrics`，端口可通过 `--metrics` 自定义 | vmagent 直接抓取 |
| 社区版支持 PostgreSQL 作为元数据引擎，默认使用 SSL 连接 | 若数据库未启用 SSL 需在 URL 加 `sslmode=disable` |
| 默认启用回收站（`.trash`），删除文件在到期后清理；后台清理依赖至少一个在线挂载点 | trash-days 必须覆盖 PITR 窗口 |
| `juicefs gc --delete` 会清理泄漏对象 | 仅在维护窗口执行，且必须在 PITR 窗口外 |

### MinIO 侧

| 事实 | 影响 |
|---|---|
| 开启 Versioning 后写入生成新版本，旧版本保留 | 用于备份 bucket 防误删 |
| Object Lock（WORM）使对象在保留期内不可变更/删除，按版本生效 | 用于备份不可变性保障 |

### PostgreSQL 侧

| 事实 | 影响 |
|---|---|
| PITR 需要连续完整的 WAL 归档序列 + 基础备份 | 必须在第一次基础备份前建立归档流程 |
| `archive_timeout` 控制未归档窗口 | 设为 60s，平衡 RPO 与归档存储成本 |
| `recovery.signal` + `restore_command` 触发归档恢复 | PITR 恢复的标准流程 |
| Advisory lock：会话级跨事务持有直到显式释放或会话结束 | WVS 使用事务级 advisory lock 避免泄漏 |
| `FOR UPDATE SKIP LOCKED` 跳过已锁行，适合队列消费 | 必须配合状态字段更新，不依赖锁本身传递状态 |

### VictoriaMetrics 侧

| 组件 | 默认端口 | 用途 |
|---|---:|---|
| vmagent | 8429 | 抓取指标 + remote_write（MVP） |
| vmsingle | 8428 | 单机指标存储 + 查询（MVP） |
| vmauth | 8427 | HTTP 代理/鉴权/路由（GA） |
| vmalert | 8880 | 规则评估 + 告警（GA） |

### 外部依据（2026-02-17 核验）

1. RFC 2119（规范性术语）：https://datatracker.ietf.org/doc/html/rfc2119
2. PostgreSQL `FOR UPDATE SKIP LOCKED`：https://www.postgresql.org/docs/current/sql-select.html
3. JuiceFS `clone`（目录 clone 一致性与并发注意事项）：https://juicefs.com/docs/community/guide/sync_dir/
4. JuiceFS 回收站与 `trash-days`：https://juicefs.com/docs/community/guide/trash/
5. MinIO Object Lock（WORM）：https://docs.min.io/enterprise/aistor-object-store/administration/object-management/object-locking/

---

## docs/README.md

````markdown
# WVS 文档集（Workspace Versioning Service）

## 快速导航

| 文件 | 内容 |
|---|---|
| 00-executive-summary.md | 目标、功能清单、SLA/RPO/RTO、约束与假设 |
| 01-architecture.md | 总体架构、拓扑、时序图、部署图 |
| 02-components.md | 组件说明与职责边界 |
| 03-network-and-auth.md | 网络、端口、TLS、鉴权与最小权限 |
| 04-api-and-protocols.md | HTTP API（OpenAPI）+ gRPC proto + quiesce 协议 + webhook |
| 05-data-model-ddl.md | 数据模型与 PostgreSQL DDL |
| 06-task-engine.md | 任务状态机、并发与幂等、重试 |
| 07-runbooks.md | 操作流程与 Runbook |
| 08-pitr-orchestration.md | PITR 编排与 MinIO 不可变策略 |
| 09-observability.md | Metrics、日志、OTel traces |
| 10-vmalert-rules.md | vmalert 告警规则 |
| 11-grafana-dashboards.md | Grafana 看板 |
| 12-cicd-and-deployment.md | docker-compose demo + K8s manifests |
| 13-dev-guide.md | 开发指南 |

说明：
- MVP 交付最小集合：`00,01,02,03,04,05,06,07,09(基础可观测),12(仅 docker-compose + 基础 CI)`
- GA 扩展集合：`08,09(高级 traces/告警运营),10,11,12(K8s 生产化与完整发布门禁)`

## 目录约定（强制）

```
/ws/<wsid>/
  current -> live/<currentId>         # symlink，agent 唯一访问入口
  live/<currentId>/...                 # 可写工作树（current 指向）
  snapshots/<snapshotId>/...           # 只读快照树（clone 产生）
  .wvs/
    control.json                      # quiesce 文件协议
    snapshot.json                     # 快照元信息
```

**agent 永远且只能访问 `/ws/<wsid>/current`。**

## 参考实现优先级

- **Demo**：单 executor + 单机 vmsingle + docker-compose
- **生产**：多 executor + 高可用 Postgres + 可选 VM Cluster + vmauth 统一入口
````

---

## docs/00-executive-summary.md

````markdown
# Executive Summary

## 背景与目标

WVS 提供面向"AI agent 工作目录（workspace）"的版本控制能力，对象为 POSIX 目录树（非文件级 diff/merge）。

核心操作：

| 操作 | 说明 |
|---|---|
| snapshot | 为当前目录树创建只读快照 |
| set_current | 将 `current` 切换到指定 snapshot 派生的新可写工作树 |
| drop snapshot | 删除指定快照（受引用保护） |

## 底层存储

| 组件 | 角色 | 实例数 |
|---|---|---|
| JuiceFS 社区版 | POSIX 文件系统（FUSE 挂载） | 1 个 volume |
| MinIO | 对象存储（S3 兼容） | 1 集群 |
| PG(juicefs-meta) | JuiceFS 元数据引擎 | 1 实例（生产主从） |
| PG(wvs) | WVS 控制平面数据库 | 1 实例（生产主从） |
| 可观测栈（MVP: vmagent + vmsingle；GA: vmauth/vmalert/Grafana） | 监控与告警 | 见部署章节 |

## 功能清单

### MVP（必须实现）

1. Workspace 生命周期：create / get / list / retry-init / disable（软删除）
2. 快照模型：snapshot 集合 + current 指针
3. snapshot create/list/drop + set_current API（异步任务化）
4. quiesce 文件协议：**强制实施**（因 clone 对目录非原子）
5. 原子切换 current（symlink rename）
6. 任务队列：PostgreSQL SKIP LOCKED + advisory lock + 指数退避重试
7. 幂等：所有异步写操作强制 `Idempotency-Key` + `request_hash` 校验
8. 审计：全量记录到 audit 表
9. 可观测工具：Prometheus 指标 + 结构化日志 + vmagent/vmsingle（MVP 必选）
10. Demo：docker-compose 一键拉起并完成全链路功能演示
11. 控制台 UXUI：CLI/TUI 展示并可操作全部主要功能（MVP 必选）

### GA（生产增强，MVP 之后）

1. executor 多副本与调度策略（hash(wsid) % N）
2. API 认证：JWT + OIDC（外接 IdP）
3. RBAC（tenant / workspace 级）
4. PITR 编排（双库）+ 演练验证
5. MinIO Object Lock 用于备份不可变
6. 快照一致性重建工具（wvs-index）
7. 资源配额：workspace inode/size 限制
8. K8s 安全加固：NetworkPolicy + Pod Security Admission

### 非目标（明确不做）

- **不做**跨文件系统/跨 bucket 的快照复制
- **不做**内容级差异（diff）与合并（merge）
- **不做** agent 业务逻辑改造（仅提供文件协议 + webhook）
- **不做**多 JuiceFS volume 支持
- **不做**实时文件变更通知/watch
- **不做**浏览器 Web UI（必须提供控制台 UXUI）

## SLA / RPO / RTO（分层目标）

| 指标 | 目标 | 说明 |
|---|---|---|
| wvs-api 可用性 | MVP: 单实例可用；GA: 99.9%（月） | GA 基于多副本无状态部署 |
| 任务成功率 | 99.95% | 含自动重试 |
| snapshot/set_current RPO | ≈0 | 基于 clone，不依赖异步备份 |
| PITR RPO | GA: ≤ archive_timeout（60s） | 取决于 WAL 归档延迟 |
| workspace set_current RTO | 秒级 | 取决于目录项数量 |
| PITR RTO | GA: 分钟~小时级 | 取决于 base backup 大小 + WAL 回放量 |

## MVP 固化实现决策（禁止自由发挥）

1. `POST /v1/workspaces` 返回 `202`，workspace 初始状态为 `PROVISIONING`，初始化任务成功后转 `ACTIVE`，失败转 `INIT_FAILED`
2. `snapshot` 与 `set_current` 必须在 quiesce `FROZEN` ack 成功后执行；超时即任务失败（fail-closed）
3. 所有异步写 API 缺失 `Idempotency-Key` 一律返回 `400 WVS_BAD_REQUEST`
4. 数据库迁移只允许由独立 migration job 执行；业务进程启动时禁止自动迁移
5. 所有 worker 出队必须覆盖 `PENDING` 与可重试 `FAILED`，不得创建第二套重试队列
6. drop snapshot 必须执行引用保护：当前指针与 PENDING/RUNNING/FAILED(可重试) 任务引用的快照禁止删除
7. MVP 必须交付控制台 UXUI（CLI/TUI），可无浏览器 Web UI

## 技术栈

| 层 | 选型 | 理由 |
|---|---|---|
| 开发语言 | **Go** | 并发模型、子进程管理、Prometheus/OTel 生态成熟、单体二进制交付 |
| HTTP 框架 | **chi** | 轻量、标准 net/http 兼容、中间件链清晰 |
| DB 访问 | **pgx + sqlc** | 类型安全、生成代码、直接控制 SQL |
| 数据库迁移 | **golang-migrate** | SQL 文件驱动、CI 友好 |
| gRPC | **grpc-go** | worker↔executor 通信 |
| 指标 | **prometheus/client_golang** | 标准 Prometheus 客户端 |
| 链路追踪 | **go.opentelemetry.io/otel** | OTel 标准 |
| 日志 | **zap** | 结构化 JSON、高性能 |
| 配置 | **环境变量 + envconfig** | 12-factor、容器友好 |
| ID 生成 | **UUID v7** | 时间有序、PostgreSQL 18 原生支持、存为 `TEXT` 类型 |
````

---

## docs/01-architecture.md

````markdown
# 总体架构与拓扑

## 核心模型（MVP）

- `workspace.current`：当前可写入口
- `snapshot`：从 current 派生的只读快照
- `set_current(snapshot_id)`：从目标 snapshot clone 出新 current 并原子切换

## 五面模型

| 面 | 组件 | 职责 |
|---|---|---|
| 控制面 | wvs-api | 对外 API、审计、任务创建 |
| 调度面 | wvs-worker | 队列消费、并发互斥、重试 |
| 执行面 | executor | clone、quiesce、current 原子切换 |
| 数据面 | agent sandbox | 只读写 `/ws/<wsid>/current` |
| 观测面 | vmagent + vmsingle（MVP） | 指标采集、存储与查询 |

## 关键时序

### Snapshot Create

1. API 创建 `snapshot_create` 任务
2. worker 获取 workspace 锁
3. executor quiesce -> clone current 到 snapshots 目录
4. 任务成功并写审计

### Set Current

1. API 创建 `set_current` 任务（参数 snapshot_id）
2. worker 获取 workspace 锁
3. executor quiesce -> clone snapshot 到新的 live 目录 -> 原子替换 `current` symlink
4. worker 更新 `workspaces.current_snapshot_id` 和 `workspaces.current_path`

### Snapshot Drop

1. API 创建 `snapshot_drop` 任务
2. worker 获取 workspace 锁
3. worker 在持锁事务内校验引用保护（current/PENDING/RUNNING/FAILED(可重试)），校验通过后在**同一事务内**将 `snapshots.deleted_at` 置为 `now()`
4. executor 异步删除目标快照目录；若目录删除失败，记录告警但**不回滚** `deleted_at`（目录残留可由后续清理任务处理）
````

---

## docs/02-components.md

````markdown
# 组件说明与职责边界

## wvs-api

- 提供 workspace/snapshot/current/task API
- 请求校验、幂等、审计记录
- 仅访问 PG(wvs)

## wvs-worker

- 消费 `tasks`
- 基于 `pg_advisory_xact_lock` 做 workspace 串行化
- 调用 executor 执行数据面操作

## executor

- 执行 quiesce
- 执行 clone
- 原子切换 current
- 不直接访问 PG(wvs)

## agent sandbox

- 只读写 `/ws/<wsid>/current`
- 必须实现 quiesce watcher

## 最小权限边界

| 组件 | 可访问 | 不可访问 |
|---|---|---|
| wvs-api | PG(wvs) | MinIO, PG(juicefs-meta), JuiceFS |
| wvs-worker | PG(wvs), executor | MinIO, PG(juicefs-meta), JuiceFS |
| executor | JuiceFS mount (/ws) | PG(wvs) |
````
## docs/03-network-and-auth.md

````markdown
# 网络、端口、TLS 与鉴权

> 分层执行：MVP 要求最小可用安全闭环；鉴权、生产级 mTLS、NetworkPolicy 在 GA 强制。

## 端口规划

| 组件 | 端口 | 协议 | 方向 | 说明 |
|---|---:|---|---|---|
| wvs-api | 8080 | HTTP/HTTPS | 外部→API | MVP 可内网 HTTP；GA 外部入口 HTTPS |
| wvs-api | 9090 | HTTP | 内部 | /metrics + /healthz + /readyz |
| wvs-worker | 9091 | HTTP | 内部 | /metrics + /healthz |
| executor | 7070 | gRPC/gRPC mTLS | worker→executor | MVP 可内网 gRPC；GA 强制 gRPC mTLS |
| executor | 9092 | HTTP | 内部 | /metrics + /healthz |
| JuiceFS mount | 9567 | HTTP | 内部 | mount 指标 |
| PG(wvs) | 5432 | TCP/TLS | 内部 | MVP 可禁用 TLS；GA 强制 TLS |
| PG(juicefs-meta) | 5432 | TCP/TLS | 内部 | MVP 可禁用 TLS；GA 强制 TLS |
| MinIO | 9000 | HTTP/HTTPS | 内部 | MVP 可 HTTP；GA 强制 HTTPS |
| MinIO Console | 9001 | HTTPS | 内部 | 管理台（生产可禁用） |
| vmagent | 8429 | HTTP | 内部 | scrape 管理 |
| vmsingle | 8428 | HTTP | 内部 | 查询 + remote_write |
| vmalert | 8880 | HTTP | 内部 | 规则评估 UI |
| vmauth | 8427 | HTTP/HTTPS | 内部 | GA 统一入口（生产建议 HTTPS） |
| Grafana | 3000 | HTTP/HTTPS | 内部/外部 | GA 看板（生产建议 HTTPS） |
| Alertmanager | 9093 | HTTP | 内部 | 告警接收 |

## TLS 策略

| 链路 | MVP 要求 | GA 要求 |
|---|---|---|
| 外部 → wvs-api | 允许 Ingress 终止 TLS 或内网 demo HTTP | **强制 HTTPS** |
| worker → executor | 允许内网明文 gRPC（受限网络） | **强制 gRPC mTLS**（双向证书） |
| * → PG(wvs) | 支持 TLS，demo 可 `sslmode=disable` | **强制 TLS** |
| * → PG(juicefs-meta) | 支持 TLS，demo 可 `sslmode=disable` | **强制 TLS** |
| * → MinIO | demo 可 HTTP | **强制 HTTPS** |
| Demo 环境 | 全部可用 HTTP/无 TLS | 不适用 |

## API 鉴权

### MVP：无鉴权

- 不校验 `Authorization` 头
- 网络访问控制通过内网隔离、入口白名单和最小权限边界保障

### GA：JWT + OIDC（强制）

- wvs-api 作为 Resource Server 验证 JWT（通过 JWKS endpoint）
- Token claims 中提取 `sub`（用户）、`tenant`（租户）
- RBAC 规则存储在 PG(wvs)（GA 阶段设计，MVP 不实现）

## vmauth 配置（GA）

```yaml
# configs/vmauth/auth.yml
users:
  - username: "grafana"
    password: "${VMAUTH_GRAFANA_PASSWORD}"
    url_prefix:
      - "http://vmsingle:8428/"
    allowed_paths:
      - "/api/v1/query"
      - "/api/v1/query_range"
      - "/api/v1/series"
      - "/api/v1/labels"
      - "/vmui"
  - username: "vmalert"
    password: "${VMAUTH_VMALERT_PASSWORD}"
    url_prefix:
      - "http://vmsingle:8428/"
```

## 最小权限边界

| 组件 | 可访问 | 不可访问 |
|---|---|---|
| wvs-api | PG(wvs) | MinIO, PG(juicefs-meta), JuiceFS |
| wvs-worker | PG(wvs), executor (gRPC) | MinIO, PG(juicefs-meta), JuiceFS |
| executor | JuiceFS mount (/ws), PG(juicefs-meta)*, MinIO* | PG(wvs) |
| grafana / vmalert | vmsingle (via vmauth, 只读) | PG, MinIO, JuiceFS |

\* executor 通过 JuiceFS mount 间接使用，不直接连接

## K8s NetworkPolicy（GA 必选，MVP 非阻塞）

```yaml
# wvs namespace: 只允许 Ingress → wvs-api:8080
# wvs namespace: 只允许 worker → executor:7070
# wvs namespace: 只允许 wvs-api, wvs-worker → PG(wvs):5432
# storage namespace: 只允许 executor/agent → PG(juicefs-meta):5432
# storage namespace: 只允许 executor/agent → MinIO:9000
```

具体 YAML 见 12-cicd-and-deployment.md。
````

---

## docs/04-api-and-protocols.md

````markdown
# API、协议与合约

## HTTP API（OpenAPI 3.0）

### 通用约定

**Base URL:**  
- MVP demo: `http://wvs.example.local`（内网/开发环境）  
- GA/生产: `https://wvs.example.com`

**请求头：**
| Header | 必须 | 说明 |
|---|---|---|
| Authorization | MVP 否；GA 是 | GA 使用 `Bearer <JWT>` |
| Idempotency-Key | 所有异步写接口必填 | 客户端生成的唯一键（UUID v7） |
| X-Request-Id | 否 | 客户端请求 ID，用于追踪（服务端会自动生成并在响应中返回） |

**分页：** 所有 list 接口支持游标分页

| 参数 | 类型 | 默认 | 说明 |
|---|---|---|---|
| cursor | string | 空 | 上一页返回的 next_cursor |
| limit | integer | 20 | 每页数量，最大 100 |

**错误码（完整列表）：**

| 错误码 | HTTP Status | 说明 |
|---|---|---|
| WVS_BAD_REQUEST | 400 | 参数校验失败 |
| WVS_UNAUTHORIZED | 401 | 未认证（GA 鉴权启用时） |
| WVS_FORBIDDEN | 403 | 无权限（GA 鉴权启用时） |
| WVS_NOT_FOUND | 404 | 资源不存在 |
| WVS_CONFLICT_LOCKED | 409 | workspace 正在被其他任务锁定 |
| WVS_CONFLICT_IDEMPOTENT_MISMATCH | 409 | 同一 idempotency_key 但请求内容不同 |
| WVS_CONFLICT_EXISTS | 409 | 资源已存在 |
| WVS_CONFLICT_SNAPSHOT_IN_USE | 409 | snapshot 正在被 current 或 PENDING/RUNNING/FAILED(可重试) 任务引用 |
| WVS_GONE | 410 | workspace 已禁用 |
| WVS_PRECONDITION_FAILED | 412 | 前置条件不满足（例如 workspace 未 ACTIVE） |
| WVS_INTERNAL | 500 | 服务内部错误 |
| WVS_EXECUTOR_ERROR | 502 | executor 执行失败 |
| WVS_EXECUTOR_TIMEOUT | 504 | executor 执行超时 |

### API 端点

#### 健康检查

```
GET /healthz
GET /readyz
GET /metrics
```

#### Workspace

```yaml
POST /v1/workspaces
  请求头: Idempotency-Key (必填)
  请求体: { wsid, owner }
  响应 202: { wsid, state: "PROVISIONING", init_task_id }

POST /v1/workspaces/{wsid}/retry-init
  请求头: Idempotency-Key (必填)
  前置条件: state = INIT_FAILED
  规则: 创建 op=init_workspace 类型任务，workspace state 重置为 PROVISIONING
  响应 202: { task_id, status: "PENDING" }

GET /v1/workspaces
  查询参数: cursor?, limit?
  响应 200: { items: [...], next_cursor?: string }
GET /v1/workspaces/{wsid}

DELETE /v1/workspaces/{wsid}
  前置条件: 无 PENDING/RUNNING/FAILED(可重试) 任务
  响应 200: { wsid, state: "DISABLED", updated_at }
  幂等行为: 重复 DELETE 已 DISABLED 的 workspace 幂等返回 200
  禁用后行为规则（强制）:
    - 所有 GET 接口（workspace/snapshot/task/current）正常返回（只读查询）
    - 所有写操作（snapshot create/drop、set_current、retry-init）返回 WVS_GONE 410
    - cancel 已终态的任务返回原状态（不变）
```

#### Snapshot

```yaml
POST /v1/workspaces/{wsid}/snapshots
  请求头: Idempotency-Key (必填)
  请求体: { message? }
  前置条件: workspace state = ACTIVE
  响应 202: { task_id, status: "PENDING" }

GET /v1/workspaces/{wsid}/snapshots
  查询参数: cursor?, limit?
  规则: 仅返回 `deleted_at IS NULL` 的快照
  响应 200:
    items:
      - snapshot_id: string
        message: string
        fs_path: string
        created_at: string
    next_cursor: string | null

DELETE /v1/workspaces/{wsid}/snapshots/{snapshot_id}
  请求头: Idempotency-Key (必填)
  前置条件:
    - snapshot 存在
    - snapshot != workspace.current_snapshot_id
    - snapshot 未被 PENDING/RUNNING/FAILED(可重试) 任务引用
  响应 202: { task_id, status: "PENDING" }
```

#### Current

```yaml
GET /v1/workspaces/{wsid}/current
  响应 200:
    data:
      wsid: string
      current_path: string
      current_snapshot_id: string | null
      updated_at: string

POST /v1/workspaces/{wsid}/current:set
  请求头: Idempotency-Key (必填)
  请求体: { snapshot_id: string }
  前置条件:
    - workspace state = ACTIVE
    - snapshot_id 属于该 workspace 且 `deleted_at IS NULL`
  规则:
    - 若 snapshot_id == current_snapshot_id，任务应以 `SUCCEEDED(noop=true)` 完成，不执行 clone
  响应 202: { task_id, status: "PENDING" }
```

#### Task

```yaml
GET /v1/tasks
  查询参数: cursor?, limit?, wsid?, status?, op?
  响应 200: { items: [...], next_cursor?: string }
GET /v1/tasks/{task_id}

POST /v1/tasks/{task_id}:cancel
  前置条件: status IN (PENDING, RUNNING)
  规则:
    - PENDING 任务直接置为 CANCELED
    - RUNNING 任务标记 cancel_requested=true，由 worker 在下次检查时中止并置为 CANCELED
    - 已终态任务（SUCCEEDED/FAILED/DEAD/CANCELED）返回 WVS_CONFLICT_LOCKED
  响应 200: { task_id, status: "CANCELED" | "RUNNING", cancel_requested: bool }

Task.op:
  - init_workspace
  - snapshot_create
  - snapshot_drop
  - set_current
```

#### Webhook（GA）

```yaml
POST /v1/webhooks
DELETE /v1/webhooks/{webhook_id}
```

## gRPC Proto（worker ↔ executor）

`ExecuteTask` 保留 4 类任务：

- `INIT_WORKSPACE`
- `SNAPSHOT_CREATE`
- `SNAPSHOT_DROP`
- `SET_CURRENT`

## Quiesce 文件协议

### 为什么 quiesce 是强制的

JuiceFS `clone` 对目录不是原子操作。snapshot create 与 set_current 前必须完成 quiesce。

### 文件路径

`/ws/<wsid>/.wvs/control.json`

### 状态机

`RUNNING -> REQUEST_FREEZE -> FROZEN -> REQUEST_RESUME -> RUNNING`

### 超时策略

- quiesce 等待 agent ack 的超时由 `EXECUTOR_QUIESCE_TIMEOUT` 控制（默认 30s）
- `EXECUTOR_QUIESCE_TIMEOUT` **MUST** 小于 `EXECUTOR_TASK_TIMEOUT`，确保超时后仍有时间执行清理
- 超时即任务失败：`QUIESCE_TIMEOUT`
- 禁止进入 clone 阶段（fail-closed）

## Audit Event Schema

audit payload 最少包含：

- `action`: `workspace.snapshot_create` / `workspace.snapshot_drop` / `workspace.set_current`
- `wsid`
- `request_id`
- `task_id`
- `before`
- `after`
- `error`
````
## docs/05-data-model-ddl.md

````markdown
# 数据模型与 DDL

> 目标数据库：PG(wvs)，schema = wvs

## ER 关系（Snapshot + Current）

```
workspaces 1──N snapshots
workspaces 1──N tasks
```

## DDL

```sql
CREATE SCHEMA IF NOT EXISTS wvs;

CREATE TABLE wvs.workspaces (
  wsid                TEXT PRIMARY KEY CHECK (wsid ~ '^[a-z0-9][a-z0-9-]{1,61}[a-z0-9]$'),
  root_path           TEXT NOT NULL UNIQUE,
  owner               TEXT NOT NULL,
  state               TEXT NOT NULL DEFAULT 'PROVISIONING'
                      CHECK (state IN ('PROVISIONING', 'ACTIVE', 'INIT_FAILED', 'DISABLED')),
  current_snapshot_id TEXT,
  current_path        TEXT NOT NULL,
  created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at          TIMESTAMPTZ NOT NULL DEFAULT now()
);

CREATE TABLE wvs.snapshots (
  snapshot_id         TEXT PRIMARY KEY,
  wsid                TEXT NOT NULL REFERENCES wvs.workspaces(wsid),
  fs_path             TEXT NOT NULL,
  message             TEXT,
  created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),
  deleted_at          TIMESTAMPTZ
);

CREATE INDEX idx_snapshots_wsid_created ON wvs.snapshots(wsid, created_at DESC);
CREATE UNIQUE INDEX uq_snapshots_wsid_fspath ON wvs.snapshots(wsid, fs_path);

ALTER TABLE wvs.workspaces
  ADD CONSTRAINT fk_workspaces_current_snapshot
  FOREIGN KEY (current_snapshot_id)
  REFERENCES wvs.snapshots(snapshot_id);

CREATE TABLE wvs.tasks (
  task_id             TEXT PRIMARY KEY,
  wsid                TEXT NOT NULL REFERENCES wvs.workspaces(wsid),
  op                  TEXT NOT NULL CHECK (op IN ('init_workspace', 'snapshot_create', 'snapshot_drop', 'set_current')),
  status              TEXT NOT NULL DEFAULT 'PENDING'
                      CHECK (status IN ('PENDING', 'RUNNING', 'SUCCEEDED', 'FAILED', 'CANCELED', 'DEAD')),
  idempotency_key     TEXT NOT NULL,
  request_hash        TEXT NOT NULL,
  created_at          TIMESTAMPTZ NOT NULL DEFAULT now(),
  started_at          TIMESTAMPTZ,
  ended_at            TIMESTAMPTZ,
  attempt             INT NOT NULL DEFAULT 0,
  max_attempts        INT NOT NULL DEFAULT 5,
  next_run_at         TIMESTAMPTZ NOT NULL DEFAULT now(),
  timeout_seconds     INT NOT NULL DEFAULT 300,
  cancel_requested    BOOLEAN NOT NULL DEFAULT false,
  params              JSONB NOT NULL DEFAULT '{}'::jsonb,
  result              JSONB,
  error               JSONB
);

CREATE UNIQUE INDEX uq_tasks_idempotency ON wvs.tasks(wsid, op, idempotency_key);
CREATE INDEX idx_tasks_dequeue ON wvs.tasks(status, next_run_at, created_at)
  WHERE status IN ('PENDING', 'FAILED');
CREATE INDEX idx_tasks_wsid ON wvs.tasks(wsid, created_at DESC);

CREATE TABLE wvs.audit (
  event_id            BIGSERIAL PRIMARY KEY,
  ts                  TIMESTAMPTZ NOT NULL DEFAULT now(),
  wsid                TEXT,
  actor               JSONB NOT NULL,
  action              TEXT NOT NULL,
  request_id          TEXT,
  task_id             TEXT,
  payload             JSONB NOT NULL
);

CREATE INDEX idx_audit_wsid_ts ON wvs.audit(wsid, ts DESC);
CREATE INDEX idx_audit_task ON wvs.audit(task_id) WHERE task_id IS NOT NULL;
```

## 任务出队 SQL（SKIP LOCKED）

```sql
WITH picked AS (
  SELECT task_id
  FROM wvs.tasks
  WHERE status IN ('PENDING', 'FAILED')
    AND next_run_at <= now()
    AND attempt < max_attempts
  ORDER BY created_at
  FOR UPDATE SKIP LOCKED
  LIMIT 1
)
UPDATE wvs.tasks t
SET status = 'RUNNING', started_at = now(), attempt = attempt + 1
FROM picked
WHERE t.task_id = picked.task_id
RETURNING t.*;
```

## Advisory Lock 使用方式

```sql
SELECT pg_advisory_xact_lock(hashtext($1));  -- $1 = wsid
```

## 迁移文件

```
migrations/
  000001_init_schema.up.sql
  000001_init_schema.down.sql
```
````
## docs/06-task-engine.md

````markdown
# 任务状态机、幂等与并发

## 状态机

```mermaid
stateDiagram-v2
  [*] --> PENDING
  PENDING --> RUNNING
  RUNNING --> SUCCEEDED
  RUNNING --> FAILED
  FAILED --> RUNNING: next_run_at 到达且 attempt < max_attempts
  FAILED --> DEAD: attempt >= max_attempts
  PENDING --> CANCELED
  RUNNING --> CANCELED
```

**任务类型（MVP）：**

- `init_workspace`
- `snapshot_create`
- `snapshot_drop`
- `set_current`

## 幂等策略

1. 所有异步写接口必须携带 `Idempotency-Key`
2. `request_hash = SHA-256(sorted_json(request_body) + method + path)`，其中 `sorted_json` 为按 key 字典序递归排列的 JSON 规范化字符串（无额外空格）
3. `(wsid, op, idempotency_key)` 唯一约束去重
4. 同 key 不同 request_hash 返回 `WVS_CONFLICT_IDEMPOTENT_MISMATCH`

## 重试与退避

- `base=5s`
- `max_backoff=300s`
- `max_attempts=5`

## 并发互斥

- 使用 `pg_advisory_xact_lock(hashtext(wsid))`
- 同 workspace 串行执行；不同 workspace 可并行

## 关键业务规则

- `set_current` 成功后必须更新 `workspaces.current_snapshot_id` 和 `workspaces.current_path`
- `set_current` 若目标即当前快照，必须以 no-op 成功返回，不执行 clone
- `snapshot_drop` 必须校验：不能删除当前指向快照，不能删除 PENDING/RUNNING/FAILED(可重试) 任务引用快照
- `snapshot_drop` 必须按"先在持锁事务内标记 `deleted_at`、后异步删除目录"执行；引用检查与 `deleted_at` 写入 MUST 在同一 advisory lock 事务内完成；目录删除失败时记录告警但不回滚 `deleted_at`（目录残留由清理任务处理）
- `INIT_WORKSPACE` 达到最大重试后必须置 `workspaces.state=INIT_FAILED`
````

---

## docs/07-runbooks.md

````markdown
# 操作流程与 Runbook

## Snapshot Create

```bash
curl -X POST http://wvs.example.local/v1/workspaces/ws-1/snapshots \
  -H "Idempotency-Key: snap-20260217-001" \
  -H "Content-Type: application/json" \
  -d '{"message":"before experiment"}'
```

## Set Current

```bash
curl -X POST http://wvs.example.local/v1/workspaces/ws-1/current:set \
  -H "Idempotency-Key: setcur-20260217-001" \
  -H "Content-Type: application/json" \
  -d '{"snapshot_id":"snap-01JB..."}'
```

## Snapshot Drop

```bash
curl -X DELETE http://wvs.example.local/v1/workspaces/ws-1/snapshots/snap-01JB... \
  -H "Idempotency-Key: drop-20260217-001"
```

## 故障排查

```bash
# 查看任务
curl http://wvs.example.local/v1/tasks/{task_id}

# 查看 executor 指标
curl http://executor:9567/metrics | grep juicefs_uptime
```
````
## docs/08-pitr-orchestration.md

````markdown
# PITR 编排与 MinIO 不可变策略

> 此章节为 **GA 阶段**内容。MVP 阶段只需确保 WAL 归档已启用。

## 为什么必须双库 PITR

| 只恢复 | 后果 |
|---|---|
| PG(juicefs-meta) | 文件系统回到过去，但 WVS 的 current/snapshot 元数据仍指向未来状态 |
| PG(wvs) | 元数据回到过去，但文件系统包含元数据未知的新快照目录 |

**结论：** 两库必须恢复到同一时间点（或 PG(wvs) 略早于 PG(juicefs-meta)），然后运行 wvs-index 修复差异。

## MinIO 备份 bucket 配置

```bash
# 开启 versioning
mc version enable myminio/pg-backup

# 开启 Object Lock（创建 bucket 时必须启用，事后无法开启）
mc mb --with-lock myminio/pg-backup

# 设置保留策略（例如 14 天）
mc retention set --default COMPLIANCE 14d myminio/pg-backup
```

## WAL-G PITR 方案（标准）

### 环境变量

```bash
export WALG_S3_PREFIX="s3://pg-backup/wvs"         # PG(wvs)
# export WALG_S3_PREFIX="s3://pg-backup/jfs-meta"  # PG(juicefs-meta) 用另一套
export AWS_ENDPOINT="https://minio.internal:9000"
export AWS_S3_FORCE_PATH_STYLE="true"
export AWS_ACCESS_KEY_ID="<backup-user>"
export AWS_SECRET_ACCESS_KEY="<backup-secret>"
```

### PostgreSQL 归档配置

```conf
# postgresql.conf
archive_mode = on
archive_command = 'wal-g wal-push %p'
archive_timeout = 60    # 60 秒未归档则强制切换 WAL 段
```

### 备份操作

```bash
# 基础备份（强制每日 cron）
wal-g backup-push $PGDATA

# 查看备份列表
wal-g backup-list

# 验证备份完整性
wal-g wal-verify integrity
```

### PITR 恢复步骤

```bash
# 1. 停止所有 JuiceFS mount（agent + executor）
kubectl scale deployment agent --replicas=0
kubectl scale daemonset wvs-executor --replicas=0

# 2. 停止 PG(juicefs-meta)
pg_ctl stop -D $PGDATA_JFS

# 3. 恢复 PG(juicefs-meta)
rm -rf $PGDATA_JFS/*
WALG_S3_PREFIX=s3://pg-backup/jfs-meta wal-g backup-fetch $PGDATA_JFS LATEST

cat > $PGDATA_JFS/postgresql.auto.conf <<EOF
restore_command = 'WALG_S3_PREFIX=s3://pg-backup/jfs-meta wal-g wal-fetch "%f" "%p"'
recovery_target_time = '2026-02-17 12:34:56+00'
EOF

touch $PGDATA_JFS/recovery.signal
pg_ctl start -D $PGDATA_JFS
# 等待恢复完成

# 4. 同样恢复 PG(wvs)（同样的目标时间点或略早）

# 5. 重新启动 JuiceFS mount（先只读验证）
juicefs mount "postgres://...@pg-juicefs:5432/juicefs_meta" /ws --read-only

# 6. 验证数据可读
ls /ws/ws-1/current/
ls /ws/ws-1/snapshots/

# 7. 运行 wvs-index 做快照一致性重建
wvs-index --scan-root=/ws --pg-dsn="postgres://...@pg-wvs:5432/wvs" --fix

# 8. 切回读写模式
umount /ws
juicefs mount "postgres://...@pg-juicefs:5432/juicefs_meta" /ws --metrics 0.0.0.0:9567

# 9. 恢复 executor 和 agent
kubectl scale daemonset wvs-executor --replicas=1
kubectl scale deployment agent --replicas=1
```

## JuiceFS GC 与回收站策略

| 配置 | 固定值 | 理由 |
|---|---|---|
| trash-days | 14 | 覆盖 PITR 窗口 |
| gc --delete | 仅维护窗口手动执行 | 防止清理 PITR 需要的历史块 |

**关键约束：** 恢复到 N 天前的元数据后，对象存储中对应的数据块必须仍然存在。因此 `trash-days ≥ PITR 最大恢复窗口`。

## wvs-index 快照一致性重建流程

1. 扫描 `/ws/<wsid>/live/*` 和 `/ws/<wsid>/snapshots/*`
2. 读取每个目录的 `.wvs/snapshot.json`
3. 与 PG(wvs) 中的 snapshots 表对比
4. 缺失快照记录 → INSERT（`--fix` 模式）
5. 缺失 current_snapshot_id → 从 `current` symlink 推断并修复
6. PG 中有记录但文件系统无对应目录 → 标记 `meta.orphan=true`
7. 输出报告：修复数量、孤儿数量、不一致详情
````

---

## docs/09-observability.md

````markdown
# 可观测性设计

> 分层执行：MVP 必须包含可观测性工具（metrics/logs + vmagent/vmsingle）并可在控制台完成观测；高级 traces、告警编排、看板运营化归入 GA。

## 三大支柱

| 支柱 | 方案 | 说明 |
|---|---|---|
| Metrics | Prometheus 格式 → vmagent → vmsingle | MVP 必选：所有组件暴露 /metrics 且可查询 |
| Logs | 结构化 JSON（zap） | 每行携带 request_id, task_id, wsid, trace_id |
| Traces | OTel SDK → OTLP exporter | MVP 可选：stdout；GA：OTLP + VictoriaTraces |

## MVP 可观测工具清单（必选）

1. `vmagent`：抓取 wvs-api/wvs-worker/executor/juicefs metrics
2. `vmsingle`：存储并查询指标
3. 控制台观测命令：通过 CLI 展示任务成功率、任务时延、quiesce 超时率、队列深度

## Metrics 清单

### 命名约定
- 前缀：`wvs_`
- Labels：`op`, `status`, `route`, `method`, `code`, `executor_id`
- **禁止**将 `wsid` 作为 label（高基数）——如需按 workspace 查询，使用日志

### wvs-api 指标

| 指标 | 类型 | Labels | 说明 |
|---|---|---|---|
| wvs_http_requests_total | counter | route, method, code | HTTP 请求总数 |
| wvs_http_request_duration_seconds | histogram | route, method | 请求延迟 |
| wvs_active_requests | gauge | — | 当前在途请求数 |

### wvs-worker 指标

| 指标 | 类型 | Labels | 说明 |
|---|---|---|---|
| wvs_task_total | counter | op, status | 任务完成计数 |
| wvs_task_duration_seconds | histogram | op | 任务端到端耗时 |
| wvs_task_queue_depth | gauge | — | 待处理任务数（PENDING + FAILED 可重试） |
| wvs_task_retry_total | counter | op | 重试次数 |
| wvs_lock_wait_seconds | histogram | — | advisory lock 等待时间 |
| wvs_dequeue_empty_total | counter | — | 空轮询次数（用于调整轮询间隔） |
| wvs_workspace_state_transitions_total | counter | from, to | workspace 状态变迁计数（如 PROVISIONING→ACTIVE、PROVISIONING→INIT_FAILED） |

### executor 指标

| 指标 | 类型 | Labels | 说明 |
|---|---|---|---|
| wvs_clone_duration_seconds | histogram | op | juicefs clone 耗时 |
| wvs_clone_entries_total | counter | op | clone 的目录项数 |
| wvs_clone_fail_total | counter | reason | clone 失败计数 |
| wvs_quiesce_wait_seconds | histogram | — | 等待 agent ack 耗时 |
| wvs_quiesce_timeout_total | counter | — | quiesce 超时次数 |
| wvs_switch_duration_seconds | histogram | — | symlink 切换耗时 |
| wvs_executor_active_tasks | gauge | — | 当前执行中的任务数 |

### JuiceFS mount 原生指标（需关注）

以下指标由 JuiceFS mount 进程在 `:9567/metrics` 暴露，vmagent 已配置抓取。MVP 阶段 MUST 关注以下关键指标：

| 指标 | 类型 | 说明 | 关注阈值 |
|---|---|---|---|
| juicefs_used_space | gauge | 已用存储空间（字节） | 接近 bucket 容量时告警 |
| juicefs_used_inodes | gauge | 已用 inode 数 | 快照积累时持续增长，需关注增速 |
| juicefs_uptime | gauge | mount 进程运行时间（秒） | 归零表示重启 |
| juicefs_object_request_durations_histogram_seconds | histogram | 对象存储请求延迟 | P95 > 1s 表示存储层异常 |
| juicefs_fuse_ops_durations_histogram_seconds | histogram | FUSE 操作延迟 | P95 > 500ms 表示文件系统性能退化 |
| juicefs_transaction_durations_histogram_seconds | histogram | 元数据事务延迟 | P95 > 200ms 表示元数据引擎压力大 |

## 日志格式

所有组件使用 zap JSON logger，字段约定：

```json
{
  "level": "info",
  "ts": "2026-02-17T12:00:00.000Z",
  "caller": "worker/dequeue.go:42",
  "msg": "task started",
  "request_id": "req-01JA...",
  "task_id": "task-01JA...",
  "wsid": "ws-1",
  "op": "snapshot_create",
  "trace_id": "abc123...",
  "span_id": "def456..."
}
```

**必须携带的字段（MVP）：** request_id, task_id (如有), wsid (如有)  
**必须携带的字段（GA）：** request_id, task_id (如有), wsid (如有), trace_id

## vmagent 配置

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: wvs-api
    static_configs:
      - targets: ["wvs-api:9090"]
  - job_name: wvs-worker
    static_configs:
      - targets: ["wvs-worker:9091"]
  - job_name: wvs-executor
    static_configs:
      - targets: ["executor:9092"]
  - job_name: juicefs-mount
    static_configs:
      - targets: ["executor:9567"]

remote_write:
  - url: "http://vmsingle:8428/api/v1/write"
```

## OTel Traces Span 设计（GA）

| Span 名 | 组件 | 关键属性 |
|---|---|---|
| `HTTP {method} {route}` | wvs-api | wsid, op, idempotency_key |
| `worker.execute_task` | wvs-worker | task_id, op, attempt, wsid |
| `worker.acquire_lock` | wvs-worker | wsid, wait_ms |
| `executor.quiesce` | executor | wsid, timeout_ms, acked |
| `executor.clone` | executor | src_path, dst_path, entries |
| `executor.switch_current` | executor | old_path, new_path |

### Trace 导出

| 形态 | 链路 |
|---|---|
| MVP | OTel SDK → stdout exporter（日志中携带 trace_id） |
| GA | OTel SDK → OTLP → OTel Collector → VictoriaTraces |
````

---

## docs/10-vmalert-rules.md

````markdown
# vmalert 告警规则

> 此章节为 GA 扩展，不阻塞 MVP 交付。

## 分级标准

| 级别 | 含义 | 响应要求 |
|---|---|---|
| critical | 数据面不可用、用户操作中断 | 立即响应 |
| warning | 延迟/错误率升高、资源逼近阈值 | 工作时间内处理 |

## 规则 YAML

```yaml
groups:
- name: wvs.api
  interval: 30s
  rules:
  - alert: WVSApiHighErrorRate
    expr: |
      (sum(rate(wvs_http_requests_total{code=~"5.."}[5m]))
       / sum(rate(wvs_http_requests_total[5m])))
      > 0.02
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "WVS API 5xx 错误率 > 2%"

  - alert: WVSApiDown
    expr: up{job="wvs-api"} == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "wvs-api 实例不可用"

- name: wvs.worker
  interval: 30s
  rules:
  - alert: WVSTaskQueueBacklog
    expr: wvs_task_queue_depth > 100
    for: 10m
    labels:
      severity: critical
    annotations:
      summary: "任务队列堆积 > 100 持续 10 分钟"

  - alert: WVSTaskHighFailRate
    expr: |
      (sum(rate(wvs_task_total{status="FAILED"}[10m]))
       / sum(rate(wvs_task_total[10m])))
      > 0.05
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "任务失败率 > 5%"

- name: wvs.executor
  interval: 30s
  rules:
  - alert: WVSExecutorDown
    expr: up{job="wvs-executor"} == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "executor 不可用"

  - alert: WVSCheckoutP95High
    expr: |
      histogram_quantile(0.95,
        sum(rate(wvs_task_duration_seconds_bucket{op="set_current"}[10m])) by (le)
      ) > 10
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "set_current P95 延迟 > 10s"

  - alert: WVSQuiesceTimeoutHigh
    expr: rate(wvs_quiesce_timeout_total[10m]) > 0.1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "quiesce 超时频率过高"

- name: wvs.storage
  interval: 30s
  rules:
  - alert: JuiceFSMountDown
    expr: up{job="juicefs-mount"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "JuiceFS mount 指标不可抓取"
```

## vmalert 启动参数

```bash
vmalert \
  -datasource.url=http://vmsingle:8428 \
  -notifier.url=http://alertmanager:9093 \
  -rule=/etc/vmalert/rules.yml \
  -evaluationInterval=30s
```
````

---

## docs/11-grafana-dashboards.md

````markdown
# Grafana 看板

> 此章节为 GA 扩展，不阻塞 MVP 交付。

## Dashboard 清单

| Dashboard | 核心面板 |
|---|---|
| WVS Overview | API QPS、5xx 率、P95 延迟、任务成功率、队列深度 |
| WVS Operations | snapshot/set_current P50/P95、clone entries vs 耗时散点图、quiesce 超时率 |
| Storage Health | JuiceFS mount up、FUSE ops 延迟（如采集）、MinIO 请求状况 |
| PITR Health | 最近 base backup 时间、WAL 归档延迟、PITR 演练结果 |

## Panel 示例

```json
{
  "type": "timeseries",
  "title": "Checkout P95 (seconds)",
  "datasource": { "type": "prometheus", "uid": "vmauth-ds" },
  "targets": [
    {
      "expr": "histogram_quantile(0.95, sum(rate(wvs_task_duration_seconds_bucket{op=\"set_current\"}[5m])) by (le))",
      "legendFormat": "P95"
    }
  ]
}
```

## Grafana Provisioning

### datasources.yml

```yaml
apiVersion: 1
datasources:
  - name: VictoriaMetrics
    type: prometheus
    uid: vmauth-ds
    url: http://vmauth:8427
    access: proxy
    basicAuth: true
    basicAuthUser: grafana
    secureJsonData:
      basicAuthPassword: "${VMAUTH_GRAFANA_PASSWORD}"
    isDefault: true
```

### dashboards.yml

```yaml
apiVersion: 1
providers:
  - name: WVS
    type: file
    options:
      path: /var/lib/grafana/dashboards
      foldersFromFilesStructure: true
```

Dashboard JSON 文件放在 `configs/grafana/dashboards/` 目录下，由 Grafana 自动加载。
````

---

## docs/12-cicd-and-deployment.md

````markdown
# CI/CD 与部署

> 分层执行：MVP 要求 docker-compose demo + 基础 CI + 控制台 UXUI 演示；K8s 生产清单与生产发布流水线归入 GA。

## docker-compose Demo

```yaml
services:
  minio:
    image: minio/minio:RELEASE.2025-01-20T14-49-07Z
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports: ["9000:9000", "9001:9001"]
    volumes: ["minio-data:/data"]
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5

  pg-juicefs:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: juicefs
      POSTGRES_PASSWORD: juicefs_pass
      POSTGRES_DB: juicefs_meta
    ports: ["5433:5432"]
    volumes: ["pgjfs-data:/var/lib/postgresql/data"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U juicefs"]
      interval: 5s
      timeout: 3s
      retries: 5

  pg-wvs:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: wvs
      POSTGRES_PASSWORD: wvs_pass
      POSTGRES_DB: wvs
    ports: ["5434:5432"]
    volumes: ["pgwvs-data:/var/lib/postgresql/data"]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U wvs"]
      interval: 5s
      timeout: 3s
      retries: 5

  executor:
    image: ${WVS_IMAGE_REPO:-yourorg}/wvs-executor:${WVS_VERSION:-dev}
    environment:
      JFS_META_URL: "postgres://juicefs:juicefs_pass@pg-juicefs:5432/juicefs_meta?sslmode=disable"
      MINIO_ENDPOINT: "http://minio:9000"
      MINIO_ACCESS_KEY: "minioadmin"
      MINIO_SECRET_KEY: "minioadmin"
      MINIO_BUCKET: "jfs-data"
      EXECUTOR_GRPC_ADDR: "0.0.0.0:7070"
      EXECUTOR_METRICS_ADDR: "0.0.0.0:9092"
    ports: ["7070:7070", "9092:9092", "9567:9567"]
    privileged: true  # FUSE 需要
    depends_on:
      pg-juicefs: { condition: service_healthy }
      minio: { condition: service_healthy }

  wvs-migrate:
    image: ${WVS_IMAGE_REPO:-yourorg}/wvs-api:${WVS_VERSION:-dev}
    entrypoint: ["migrate", "-path", "/migrations", "-database", "postgres://wvs:wvs_pass@pg-wvs:5432/wvs?sslmode=disable", "up"]
    depends_on:
      pg-wvs: { condition: service_healthy }

  wvs-api:
    image: ${WVS_IMAGE_REPO:-yourorg}/wvs-api:${WVS_VERSION:-dev}
    environment:
      WVS_DB_DSN: "postgres://wvs:wvs_pass@pg-wvs:5432/wvs?sslmode=disable"
      WVS_LISTEN_ADDR: "0.0.0.0:8080"
      WVS_METRICS_ADDR: "0.0.0.0:9090"
      WVS_LOG_LEVEL: "info"
    ports: ["8080:8080", "9090:9090"]
    depends_on:
      wvs-migrate: { condition: service_completed_successfully }

  wvs-worker:
    image: ${WVS_IMAGE_REPO:-yourorg}/wvs-worker:${WVS_VERSION:-dev}
    environment:
      WVS_DB_DSN: "postgres://wvs:wvs_pass@pg-wvs:5432/wvs?sslmode=disable"
      EXECUTOR_ADDRS: "executor:7070"
      WVS_METRICS_ADDR: "0.0.0.0:9091"
      WVS_LOG_LEVEL: "info"
      WORKER_POLL_INTERVAL: "1s"
      WORKER_IDLE_BACKOFF: "5s"
    ports: ["9091:9091"]
    depends_on:
      wvs-migrate: { condition: service_completed_successfully }
      executor: { condition: service_started }

  vmsingle:
    image: victoriametrics/victoria-metrics:v1.108.1
    ports: ["8428:8428"]
    command: ["-storageDataPath=/vm-data", "-retentionPeriod=30d"]
    volumes: ["vm-data:/vm-data"]

  vmagent:
    image: victoriametrics/vmagent:v1.108.1
    ports: ["8429:8429"]
    volumes:
      - ./configs/vmagent/prometheus.yml:/etc/vmagent/prometheus.yml:ro
    command:
      - "-promscrape.config=/etc/vmagent/prometheus.yml"
      - "-remoteWrite.url=http://vmsingle:8428/api/v1/write"
    depends_on: [vmsingle]

  # ──────────────────────────────────────────────
  # GA 组件（以下服务为 GA 扩展，MVP 不要求部署）
  # MVP demo 可注释掉以下全部 GA 服务
  # ──────────────────────────────────────────────

  vmauth:                      # GA: 统一查询入口代理
    image: victoriametrics/vmauth:v1.108.1
    ports: ["8427:8427"]
    volumes:
      - ./configs/vmauth/auth.yml:/etc/vmauth/auth.yml:ro
    command: ["-auth.config=/etc/vmauth/auth.yml"]
    depends_on: [vmsingle]
    profiles: ["ga"]           # docker compose --profile ga up 时启用

  alertmanager:                # GA: 告警接收
    image: prom/alertmanager:v0.27.0
    ports: ["9093:9093"]
    profiles: ["ga"]

  vmalert:                     # GA: 规则评估 + 告警
    image: victoriametrics/vmalert:v1.108.1
    ports: ["8880:8880"]
    volumes:
      - ./configs/vmalert/rules.yml:/etc/vmalert/rules.yml:ro
    command:
      - "-datasource.url=http://vmsingle:8428"
      - "-notifier.url=http://alertmanager:9093"
      - "-rule=/etc/vmalert/rules.yml"
    depends_on: [vmsingle, alertmanager]
    profiles: ["ga"]

  grafana:                     # GA: 看板
    image: grafana/grafana:11.4.0
    ports: ["3000:3000"]
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - ./configs/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./configs/grafana/dashboards:/var/lib/grafana/dashboards:ro
    depends_on: [vmauth]
    profiles: ["ga"]

volumes:
  minio-data:
  pgjfs-data:
  pgwvs-data:
  vm-data:
```

## MVP Demo 验收（控制台 UXUI）

MVP demo 必须通过控制台完成以下流程（禁止依赖浏览器 UI）：

1. workspace create/get/list
2. snapshot
3. set_current
4. snapshot drop
5. task 查询与状态追踪
6. 可观测性查询（任务时延、失败率、队列深度、quiesce 超时）

控制台 UXUI 可为命令行 CLI 或 TUI，但必须覆盖以上全部流程并提供清晰状态输出。

## 自动化 Smoke Test（MVP 必选）

MVP 必须提供 `scripts/smoke-test.sh`，通过 wvsctl 执行端到端验收。脚本要求：

1. **前置条件：** docker-compose 环境已运行且所有服务健康
2. **执行流程：**
   ```bash
   #!/usr/bin/env bash
   set -euo pipefail

   API_URL="${WVS_API_URL:-http://localhost:8080}"
   WSID="smoke-test-$(date +%s)"

   echo "=== 1. Create workspace ==="
   wvsctl workspace create "$WSID" --owner smoke-test --wait
   wvsctl workspace get "$WSID" | grep -q '"ACTIVE"'

   echo "=== 2. Create snapshot ==="
   SNAP_ID=$(wvsctl snapshot create "$WSID" -m "smoke test snapshot" --wait --output json | jq -r '.snapshot_id')
   [ -n "$SNAP_ID" ] && echo "snapshot_id=$SNAP_ID"

   echo "=== 3. List snapshots ==="
   wvsctl snapshot list "$WSID" | grep -q "$SNAP_ID"

   echo "=== 4. Set current ==="
   wvsctl current set "$WSID" "$SNAP_ID" --wait
   wvsctl current get "$WSID" | grep -q "$SNAP_ID"

   echo "=== 5. Create second snapshot (for drop test) ==="
   SNAP2_ID=$(wvsctl snapshot create "$WSID" -m "to be dropped" --wait --output json | jq -r '.snapshot_id')

   echo "=== 6. Drop snapshot ==="
   wvsctl snapshot drop "$WSID" "$SNAP2_ID" --wait
   # 验证已删除快照不再出现在 list 中
   ! wvsctl snapshot list "$WSID" | grep -q "$SNAP2_ID"

   echo "=== 7. Task query ==="
   wvsctl task list --wsid "$WSID" --output json | jq '.items | length' | grep -qE '^[4-9]|^[1-9][0-9]'

   echo "=== 8. Observability query ==="
   wvsctl obs summary | grep -q "task_success_rate"

   echo "=== 9. Cleanup: disable workspace ==="
   wvsctl workspace disable "$WSID"
   wvsctl workspace get "$WSID" | grep -q '"DISABLED"'

   echo "=== ALL SMOKE TESTS PASSED ==="
   ```
3. **退出码：** 全部通过返回 0；任一步骤失败返回非零
4. **CI 集成：** `make smoke-test` 目标执行 `docker compose up -d && scripts/smoke-test.sh`

## 环境变量参考（完整）

### wvs-api

| 变量 | 必须 | 默认 | 说明 |
|---|---|---|---|
| WVS_DB_DSN | 是 | — | PostgreSQL 连接串 |
| WVS_LISTEN_ADDR | 否 | 0.0.0.0:8080 | API 监听地址 |
| WVS_METRICS_ADDR | 否 | 0.0.0.0:9090 | Metrics 监听地址 |
| WVS_AUTH_TOKEN | 否(MVP) / 是(GA) | — | GA 鉴权 token/JWT 校验相关配置 |
| WVS_LOG_LEVEL | 否 | info | debug/info/warn/error |
| WVS_SHUTDOWN_TIMEOUT | 否 | 30s | 优雅关闭超时 |

### wvs-worker

| 变量 | 必须 | 默认 | 说明 |
|---|---|---|---|
| WVS_DB_DSN | 是 | — | PostgreSQL 连接串 |
| EXECUTOR_ADDRS | 是 | — | executor 地址列表，逗号分隔 |
| WVS_METRICS_ADDR | 否 | 0.0.0.0:9091 | Metrics 监听地址 |
| WVS_LOG_LEVEL | 否 | info | 日志级别 |
| WORKER_POLL_INTERVAL | 否 | 1s | 轮询间隔 |
| WORKER_IDLE_BACKOFF | 否 | 5s | 空闲时退避间隔 |
| WORKER_SHUTDOWN_TIMEOUT | 否 | 120s | 优雅关闭超时 |

### executor

| 变量 | 必须 | 默认 | 说明 |
|---|---|---|---|
| JFS_META_URL | 是 | — | JuiceFS 元数据 URL |
| MINIO_ENDPOINT | 是 | — | MinIO 地址 |
| MINIO_ACCESS_KEY | 是 | — | MinIO access key |
| MINIO_SECRET_KEY | 是 | — | MinIO secret key |
| MINIO_BUCKET | 是 | jfs-data | 数据 bucket 名 |
| EXECUTOR_QUIESCE_TIMEOUT | 否 | 30s | 等待 agent ack FROZEN 的超时，MUST 小于 EXECUTOR_TASK_TIMEOUT |
| EXECUTOR_GRPC_ADDR | 否 | 0.0.0.0:7070 | gRPC 监听地址 |
| EXECUTOR_METRICS_ADDR | 否 | 0.0.0.0:9092 | Metrics 监听地址 |
| EXECUTOR_MOUNT_PATH | 否 | /ws | JuiceFS 挂载点 |
| EXECUTOR_TASK_TIMEOUT | 否 | 300s | 默认任务超时 |
| EXECUTOR_SHUTDOWN_TIMEOUT | 否 | 300s | 优雅关闭超时 |

## K8s Manifests（GA）

### wvs-api Deployment

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wvs-api
  namespace: wvs
spec:
  replicas: 2
  selector:
    matchLabels: { app: wvs-api }
  template:
    metadata:
      labels: { app: wvs-api }
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
      - name: api
        image: yourorg/wvs-api:1.0.0
        ports:
        - { containerPort: 8080, name: http }
        - { containerPort: 9090, name: metrics }
        env:
        - name: WVS_DB_DSN
          valueFrom:
            secretKeyRef: { name: wvs-db, key: dsn }
        livenessProbe:
          httpGet: { path: /healthz, port: 9090 }
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          httpGet: { path: /readyz, port: 9090 }
          initialDelaySeconds: 5
          periodSeconds: 5
        resources:
          requests: { cpu: 100m, memory: 128Mi }
          limits: { cpu: 500m, memory: 256Mi }
```

### executor DaemonSet

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: wvs-executor
  namespace: wvs
spec:
  selector:
    matchLabels: { app: wvs-executor }
  template:
    metadata:
      labels: { app: wvs-executor }
    spec:
      containers:
      - name: executor
        image: yourorg/wvs-executor:1.0.0
        ports:
        - { containerPort: 7070, name: grpc }
        - { containerPort: 9092, name: metrics }
        - { containerPort: 9567, name: jfs-metrics }
        securityContext:
          privileged: true  # FUSE 需要
        env:
        - name: JFS_META_URL
          valueFrom:
            secretKeyRef: { name: jfs-meta, key: url }
        - name: MINIO_ACCESS_KEY
          valueFrom:
            secretKeyRef: { name: minio-creds, key: access-key }
        - name: MINIO_SECRET_KEY
          valueFrom:
            secretKeyRef: { name: minio-creds, key: secret-key }
        volumeMounts:
        - name: ws-data
          mountPath: /ws
          mountPropagation: Bidirectional
        resources:
          requests: { cpu: 200m, memory: 512Mi }
          limits: { cpu: "2", memory: 2Gi }
      volumes:
      - name: ws-data
        hostPath:
          path: /var/lib/wvs/ws
          type: DirectoryOrCreate
```

### NetworkPolicy（GA）

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: wvs-api-ingress
  namespace: wvs
spec:
  podSelector:
    matchLabels: { app: wvs-api }
  policyTypes: ["Ingress"]
  ingress:
  - from:
    - namespaceSelector: { matchLabels: { role: ingress } }
    ports:
    - { protocol: TCP, port: 8080 }
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: executor-from-worker
  namespace: wvs
spec:
  podSelector:
    matchLabels: { app: wvs-executor }
  policyTypes: ["Ingress"]
  ingress:
  - from:
    - podSelector: { matchLabels: { app: wvs-worker } }
    ports:
    - { protocol: TCP, port: 7070 }
```

## CI/CD（分层门禁）

| 阶段 | MVP 必选 | GA 扩展 |
|---|---|---|
| Lint | golangci-lint + buf lint | — |
| Unit Test | go test ./... | — |
| Integration Test | PG + MinIO 关键路径（可用 testcontainers） | 更大规模集成场景 |
| Contract Test | — | OpenAPI schema + gRPC proto 兼容性检查 |
| Image Build | amd64 单架构可发布镜像 | 多架构 + distroless |
| Security Scan | — | trivy + govulncheck |
| E2E Test | docker-compose + wvsctl 全链路一次 | 长稳压测与回归矩阵 |
| Deploy | docker-compose demo 交付 | Helm / Kustomize 生产发布 |

门禁规则：
1. MVP 合并门禁仅要求通过 MVP 必选阶段
2. GA/生产发布门禁要求通过 MVP 必选 + GA 扩展全部阶段
3. 生产部署必须基于带 Git SHA 的不可变镜像标签
````

---

## docs/13-dev-guide.md

````markdown
# 开发指南

## 工程结构

```
wvs/
├── cmd/
│   ├── wvs-api/
│   ├── wvs-worker/
│   ├── wvs-executor/
│   ├── wvsctl/
│   └── wvs-index/
├── internal/
│   ├── api/
│   ├── core/
│   │   ├── workspace.go
│   │   ├── snapshot.go
│   │   └── idempotency.go
│   ├── store/
│   ├── worker/
│   ├── executor/
│   │   ├── snapshot_create.go
│   │   ├── snapshot_drop.go
│   │   ├── set_current.go
│   │   ├── quiesce.go
│   │   └── switch.go
│   └── executorclient/
├── proto/
├── migrations/
└── configs/
```

## 关键实现指引

### 1. Snapshot Create

- 输入：`wsid`, `message`
- **幂等前置检查（MUST）：** 若目标 snapshot 目录已存在且 `.wvs/snapshot.json` 完整，视为已完成，直接返回成功
- 执行：quiesce -> clone current 到 snapshot 目录
- 输出：`snapshot_id`, `fs_path`

### 2. Set Current

- 输入：`wsid`, `snapshot_id`
- **幂等前置检查（MUST）：** 若 `current` symlink 已指向以目标 snapshot 派生的 live 目录，视为已完成，直接返回成功
- 执行：quiesce -> clone snapshot 到新 live 目录 -> `rename(2)` 原子替换 current
- 输出：`current_snapshot_id`, `current_path`

### 3. Snapshot Drop

- 输入：`wsid`, `snapshot_id`
- 校验：不得删除 current 指向快照，不得删除 PENDING/RUNNING/FAILED(可重试) 任务引用快照
- **幂等前置检查（MUST）：** 若目标 snapshot 目录已不存在，视为已完成，直接返回成功
- 执行：worker 在持锁事务内标记 `deleted_at`，executor 异步删除目录

### 4. Quiesce 等待

- 超时即 `QUIESCE_TIMEOUT`
- fail-closed：禁止继续 clone

## 控制台 UXUI（wvsctl，MVP 必选）

`wvsctl` 必须覆盖以下命令组：

1. `wvsctl workspace create|get|list|retry-init|disable`
2. `wvsctl snapshot create|list|drop`
3. `wvsctl current get|set`
4. `wvsctl task list|get|watch`
5. `wvsctl obs summary|latency|queue|quiesce`

输出要求：

- 默认表格视图 + `--json` 机器可读输出
- 所有写操作输出 `task_id` 与最终状态查询指令
- 失败输出必须包含标准错误码（如 `WVS_BAD_REQUEST`）
## 测试策略

| 类型 | 覆盖 | 工具 |
|---|---|---|
| 单元测试 | snapshot/current 规则、幂等计算、状态机转换、退避算法 | go test |
| 集成测试 | DB 操作、任务出队/重试、advisory lock | testcontainers (PG) |
| gRPC 测试 | executor server mock、worker dispatch | grpc-go testing |
| E2E 测试 | 全链路：snapshot create → set_current → snapshot drop + obs 查询 | docker-compose + wvsctl |

## Makefile targets

```makefile
.PHONY: generate lint test build

generate:                    ## 生成 sqlc + protobuf 代码
	sqlc generate
	buf generate

lint:                        ## 代码检查
	golangci-lint run ./...
	buf lint

test:                        ## 运行测试
	go test -race -count=1 ./...

build:                       ## 构建所有二进制
	CGO_ENABLED=0 go build -o bin/wvs-api ./cmd/wvs-api
	CGO_ENABLED=0 go build -o bin/wvs-worker ./cmd/wvs-worker
	CGO_ENABLED=0 go build -o bin/wvs-executor ./cmd/wvs-executor
	CGO_ENABLED=0 go build -o bin/wvsctl ./cmd/wvsctl
	CGO_ENABLED=0 go build -o bin/wvs-index ./cmd/wvs-index

docker:                      ## 构建 Docker 镜像
	docker build -t yourorg/wvs-api -f Dockerfile.api .
	docker build -t yourorg/wvs-worker -f Dockerfile.worker .
	docker build -t yourorg/wvs-executor -f Dockerfile.executor .

up:                          ## 启动 demo 环境
	docker compose up -d

down:                        ## 停止 demo 环境
	docker compose down -v

smoke-test:                  ## 端到端冒烟测试
	docker compose up -d --wait
	bash scripts/smoke-test.sh
```
````

---

## 仓库交付结构

```
wvs/
├── docs/
│   ├── README.md
│   ├── 00-executive-summary.md
│   ├── 01-architecture.md
│   ├── 02-components.md
│   ├── 03-network-and-auth.md
│   ├── 04-api-and-protocols.md
│   ├── 05-data-model-ddl.md
│   ├── 06-task-engine.md
│   ├── 07-runbooks.md
│   ├── 08-pitr-orchestration.md
│   ├── 09-observability.md
│   ├── 10-vmalert-rules.md
│   ├── 11-grafana-dashboards.md
│   ├── 12-cicd-and-deployment.md
│   └── 13-dev-guide.md
├── proto/
│   └── executor/v1/
│       └── executor.proto
├── configs/
│   ├── vmagent/prometheus.yml
│   ├── vmauth/auth.yml
│   ├── vmalert/rules.yml
│   └── grafana/
│       ├── provisioning/
│       │   ├── datasources/datasource.yml
│       │   └── dashboards/dashboards.yml
│       └── dashboards/
│           ├── wvs-overview.json
│           └── wvs-ops.json
├── migrations/
│   ├── 000001_init_schema.up.sql
│   └── 000001_init_schema.down.sql
├── cmd/
│   ├── wvs-api/main.go
│   ├── wvs-worker/main.go
│   ├── wvs-executor/main.go
│   ├── wvsctl/main.go
│   └── wvs-index/main.go
├── internal/
│   ├── api/
│   ├── core/
│   ├── store/
│   ├── worker/
│   ├── executor/
│   └── executorclient/
├── docker-compose.yml
├── Makefile
├── go.mod
└── go.sum
```
